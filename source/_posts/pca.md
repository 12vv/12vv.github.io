---
title: PCA--主成分分析
date: 2018-03-01
tag: [Machine Learning, Data Analysis]
categories: [机器学习]
---

`PCA`在很多方面均有应用，但是之前没有仔细探究过，最近看了一些博客和论文，做一下总结。

> `主成分分析（Principal Component Analysis，PCA）`， 是一种统计方法。通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。

<!--more-->

## 为什么需要PCA?

通俗一点说，`PCA`是一种降维的方法。我们知道，维数越大通常越难处理，在机器学习中，得到的数据维数通常都很高，处理起来很麻烦，资源消耗很大，因此对数据进行降维处理是很必要的。

但是降维就意味着信息的丢失吗？多少是有一点的。但是总有一些情况，让我们能能够在信息损失相对比较少的同时完成降维。比如：

*   如果某两个特征之间存在关联。举个比较极端的的例子，一个正方形的边长和它的面积，各属于两个特征，但是知道了边长面积肯定是确定的，那么就可以直接丢掉一列(边长或面积)。
*   如果某个维度存在并没有什么价值。这里举个比较经典的例子，就是电视转播球赛，把现场的三维转成平面的二维呈现在你眼前，减少了一维但是对于观众来说，并无太大影响。
*   ……

通过减少冗余信息，降低了维度，让之后处理数据更加容易，而有大部分有价值的信息都保留下来。而到底哪些信息是重要的？哪些可以删去？在这里还要注意：降维并不简单的值删去某个维度，大部分情况下，降维的同时`基`也改变了。那么如何选取新的`基`？这就是`PCA`解决的问题。

> 补充看到过的一个比较好的例子：
> 假设我们整理了30个人的体重，身高和IQ，放在一个矩阵中，每一列是一个样本(一个人的这三个变量)。为了便于观察可以在三维坐标中描点，每一维代表一个变量。提出问题：
> 
> *   有没有更简单的使数据可视化的方法？对于这个三维图像，能否在二维空间中描述它？
> *   那些变量间是`相互关联`的？在这个例子中，根据常识，应该认为`身高和IQ`没有必然联系，而`身高和体重`有一定的联系。
> *   有没有一个变量能够描述整个数据集？

## PCA原理分析

### 目标

先简单描述一下`PCA`要做的事。
假设有一组数$\begin{pmatrix}1 & 1 & 2 & 4 & 2 \\ 1 & 3 & 3 &4& 4\end{pmatrix}$, 先做简单处理，每个数减去均值，这样算方差的时候方便(因为要减均值)，得到$\begin{pmatrix}-1 & -1 & 0 & 2 & 0\\ -2 & 0 & 0 & 1 & 1\end{pmatrix}$
在二维坐标系中描出：

![](/images/pca/1-1.png)

因为这里只是二维的，那么要降成一维就是在这个二维平面重新找一个方向，并把这些点映射到这个方向上。试想，怎么才能找到这个方向，且不损失大部分信息呢？
容易想到，最后找到的这个方向，这些点的投影都不重叠，分隔的较远。
提出假设和目标：

*   `充分统计量(sufficient statistic)`，即当知道这些量的时候，这个分布就可以确定了，`均值`和`方差`可以看成是其充分统计量。
*   大的方差(variances)代表这个量的强动态性，就是说如果映射到新坐标上拥有大的方差，那么这个维度可以较好的反应数据的特征。
*   主成分需要`正交`，这个可以看下面关于方差和协方差的讨论，正交代表两成分相关性为0，这样坐标的选取才有意义。

如何达到这些目的呢？先看一些概念和例子。

### 基变换

若使用我们惯用的二维直角坐标系来表示这个下图这个向量。

![](/images/pca/1-2.png)

很显然是 `(3, 2)`, 这时二维空间的一组`基`是`(1, 0)`和`(0, 1)`, 容易证明二维空间中所有向量`(x, y)`都能用这组`基`来表示, 实际上 

$$\begin{bmatrix}x\\ y\\\end{bmatrix} = x*(1,0)^{T}+y*(0,1)^{T}$$

任何两个线性无关的二维向量都可以成为二维空间的一组基。

如果以 $(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$ 和$(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$为`基`(一般选取的基模为1)，如下图所示。

![](/images/pca/1-3.png)

那么这个向量就表示为 $(\frac{5}{\sqrt{2}},-\frac{1}{\sqrt{2}})$, 这是如何计算出来的呢？

想想也很简单，就是把这个向量投影到基的方向上，投影到长度就是那一维的坐标值，比如`(3, 2)`投影到$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$方向上。

![](/images/pca/1-4.png)

根据投影公式，A投影到B的矢量长度为 $|A|*cos(a)$， 其中$|A|=\sqrt{x_1^2+y_1^2}$是向量A的模，即矢量长度。
而，$A\cdot B=|A||B|cos(a)$, 而基的模通常都处理为1的，意味着 $|B| = 1$, 观察等号右边，那投影的长度不就是向量的基吗？

那么跟容易就可以得到，`(3, 2)`在基$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$的投影可以用 $3_\frac{1}{\sqrt{2}} + 2_\frac{1}{\sqrt{2}}$ 算出。

通过上面的推导，要直接算出向量在新基上的投影，可以用此向量乘以基矩阵，即

$$\begin{pmatrix}1/\sqrt{2}& 1/\sqrt{2} \\-1/\sqrt{2} & 1/\sqrt{2}\end{pmatrix}\begin{pmatrix}3 \\2\end{pmatrix}=\begin{pmatrix}5/\sqrt{2} \\-1/\sqrt{2}\end{pmatrix}$$

有m个二维向量，也可以直接算出他们在新基上的坐标，例如将 (1,1)，(2,2), (3,3) 映射到新基。

$$\begin{pmatrix}1/\sqrt{2}  & 1/\sqrt{2} \\-1/\sqrt{2} & 1/\sqrt{2}\end{pmatrix}\begin{pmatrix}1 & 2 & 3 \\1 & 2 & 3\end{pmatrix}=\begin{pmatrix}2/\sqrt{2} & 4/\sqrt{2} & 6/\sqrt{2} \\0& 0 & 0\end{pmatrix}$$

由此推广到高维的基变换。

设`X`是原始的数据向量的集合`(m*n)`，每一列是一个样本。设`Y`是一个m*n的矩阵，由`P`矩阵施予其变换而得，`P`中是新的坐标向量。

$$PX = Y$$

$$PX = \begin{bmatrix}p_{1}\\ ...\\p_{m}\end{bmatrix}\begin{bmatrix}x_{1} & ... & x_{n}\end{bmatrix}$$
$$Y =\begin{bmatrix}p_{1}x{1} &...  & p_{1}x_{n}\\ ...& ...&... \\ p_{m}x_{1} & ... & p_{m}x_{n}\end{bmatrix}$$

那么，Y中的每一列，设为$y_{i}$：

$$y_{i} = \begin{bmatrix}p_{1}x_{i} \\ ...\\ p_{m}x_{i} \end{bmatrix}$$

观察式子，点乘，正是前面推导过的，Y的每一列就是X的每一列映射到新基上的新坐标。

### 方差和协方差

![](/images/pca/1-5.png)

观察一下，对于二维的，我们看图就可以得出图(c)两个变量有高的相关性，也就是表示其中一维可以直接去掉。但是多维的无法看图判断。这时候就需要`协方差`，考察变量间的相关性。

注意：在处理之前，先将数据减去均值。这样之后计算方差时要减均值，就相当于减0，处理起来比较方便。


将向量A和B转换为： A = [a1, a2, . . . , an], B = [b1, b2, . . . , bn].
那么，协方差就可以表示为点乘形成的矩阵：

$$\sigma ^{2}_{AB} =\frac{1}{n-1}ab^T$$

拓展到多维的情况，定义

$$X = \begin{bmatrix}x_{1}\\ x_{2}\\ ...\\ x_{3}\end{bmatrix}$$

这里令 x1 = a， x2 = b，….那么，得到`协方差矩阵`：

$$C_{X} = \frac{1}{n-1}XX^{T}$$

> The most straight-forward normalization is 1/n. However, this provides a biased estimation of variance particularly for small n. It is beyond the scope of this paper to show that the proper normalization for an unbiased estimator is 1/n−1.   ——A Tutorial on Principal Component Analysis

注意1/n-1是避免偏差估计的方法，这个我没有了解过就不展开了，为了讨论方便，之后就用`1/m`代替`1/n-1`。

*   矩阵 `CX` 是一个`m*m`的对称方阵
*   矩阵 `CX` 上对角线上的元素是各元素的`方差`
*   矩阵 `CX` 上非对角线上的元素是`协方差`
举个例子，假设就两个变量，那么对应上面$X = \begin{bmatrix}x_{1}\\ x_{2}\\\end{bmatrix}$。假设把数据集中的点描到图像上是这样的：

![](/images/pca/1-6.png)

观察图像，水平方向具有比较大的方差(数据偏离平均较大)，可以想象$C_{11}$(代表x1方差)应该比较大，垂直方向上，方差较小，$C_{22}$应该较小，再看他们的相关性，应该是比较小的，那么$C_{12}$和$C_{21}$也应该比较小，可能我们算出的协方差矩阵类似于这样：

$$S=\begin{bmatrix}95 & 1\\ 1& 5\end{bmatrix}$$

假设画出来的图是这样的：

![](/images/pca/1-7.png)

类似的分析，在水平方向和初值方向的方差都挺大的，并且容易看出正相关性，那么得到的协方差矩阵应该类似于这样：

$$S=\begin{bmatrix}50 & 40\\ 40& 50\end{bmatrix}$$

可以回去看看我们的目标，降维要找到新的坐标轴，为了保留数据的重要信息，第一个新的坐标轴的选取就是方差最大的那个方向，而第二个新的坐标轴，如果选方差第二大的方向，那么不太有意义，因为那必定和第一个新坐标轴几乎重叠，那么他们是相关的，没有意义，因此应该选取与之正交的方向。

至此，还没有达到目的，还需要对协方差矩阵`对角化`。

### 对角化

先回顾一下线性代数中的知识。

*   定理：如果A是实对称矩阵($A=A^{T}$)，那么A是正交矩阵，并且可对角化，而且只有实特征值，换句话说，存在实数特征值$\lambda_{1},…\lambda_{n}$，和正交的非零向量(特征向量)$\vec{v_{1}}, …, \vec{v_{n}}$，使：
$$A\vec{v_{i}}=\lambda_{i}\vec{v_{i}}$$

注意前提，是`对称矩阵`，但是，如果A是m*n的矩阵，那么$AA^{T}$和$A^{T}A$都是对称矩阵！

*   推论：矩阵$AA^{T}$和$A^{T}A$的拥有相同的非零特征值。
*   证明：设$\vec{v}$是$A^{T}A$的特征值，那么：$$(A^{T}A)\vec{v}=\lambda\vec{v}$$两边同时左乘$A$，利用矩阵的结合律，得到：$$AA^{T}(A\vec{v})=\lambda(A\vec{v})$$即虽然$AA^{T}$和$A^{T}A$的特征向量不同，但是`特征值`相同！
这有什么用呢？比如A是一个`500*2`的矩阵，如果要算$AA^{T}$的特征值，那么就要处理`500*500`的矩阵，通过上面的定理，等价于求$A^{T}A$的特征值，那么只用处理`2*2`的矩阵！得到两个特征值，剩下的498个特征值均为0！

接下来回到PCA，首先明确，为什么需要将协方差矩阵对角化？

结合前面`基变换`(忘记了可以跳回去)的推导，和矩阵的相关知识，基变换后`X`变为`Y`，设`Y`的协方差矩阵为$C_{Y}$，设原始数据矩阵`X`的协方差矩阵为$C_{x}$，那么

$$\begin{array}{l l l}C_{Y} & = & \frac{1}{m}YY^\mathsf{T}\\& = & \frac{1}{m}(PX)(PX)^\mathsf{T}\\& = & \frac{1}{m}PXX^\mathsf{T}P^\mathsf{T} \\& = & P(\frac{1}{m}XX^\mathsf{T})P^\mathsf{T} \\& = & PC_{X}P^\mathsf{T}\end{array}$$

还记得`P`是新坐标向量的矩阵吗？

那么，只要对角化原始矩阵`X`，(这里涉及矩阵分解的知识)，所得到的`P`，满足$PCP^T$是一个对角矩阵，并且对角元素按从大到小依次排列，那么**P的前K行就是要寻找的基**(K取决于你要将数据降到几维)，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维，并满足上述优化条件。

对角化一般简单的方法就是解`特征方程`，得到`特征值`，求`特征向量`，这里就不再赘述。

### 总结

下面是如何在数据集中使用`PCA`。

1.  去数据集中n个样本构成n个m维向量 $\vec{x_{1}}, …, \vec{x_{n}}$，计算均值再减掉(这是为了方便，当然也可以计算方差的时候再减)，然后计算协方差矩阵。
2.  计算协方差矩阵的特征值，并降序排列 $ \lambda_{1}, …, \lambda_{m}$还有对应的特征向量 $ \vec{v_{1}}, …, \vec{v_{m}} $.
3.  如果一些 $\lambda_{i}$明显的大于其他的，那么意味着，降维是一种可行的处理方法，就取对应的特征向量为新的基。

## 例子

回到上面`目标`那一节的数据。

$$\begin{pmatrix}-1 & -1 & 0 & 2 & 0\\-2 & 0 & 0 & 1 & 1\end{pmatrix}$$

这是已经减去均值之后的了。

进入第二步，求解其协方差矩阵：

$$C=\frac{1}{5}\begin{pmatrix}-1 & -1 & 0 & 2 & 0 \\-2 & 0 & 0 & 1 & 1\end{pmatrix}\begin{pmatrix}-1 & -2 \\-1 & 0  \\0  & 0\\2  & 1  \\0  & 1\end{pmatrix}=\begin{pmatrix}\frac{6}{5} &\frac{4}{5} \\\frac{4}{5} & \frac{6}{5}\end{pmatrix}$$

第三步，求特征值，得到：

$$\lambda_1=2,\lambda_2=2/5$$

可以看到2和2/5还是相差挺多的。

对应的特征向量通解：

$$c_1\begin{pmatrix}1 \\1\end{pmatrix},c_2\begin{pmatrix}-1\\1\end{pmatrix}$$

其中c1,c2为任意实数，习惯上标准化，得到：

$$\begin{pmatrix}1/\sqrt{2} \\1/\sqrt{2}\end{pmatrix},\begin{pmatrix}-1/\sqrt{2} \\1/\sqrt{2}\end{pmatrix}$$

即得新基矩阵`P`：

$$P=\begin{pmatrix}1/\sqrt{2}  & 1/\sqrt{2}  \\-1/\sqrt{2} & 1/\sqrt{2}\end{pmatrix}$$

可以`验证`对角化：

$$PCP^\mathsf{T}=\begin{pmatrix}1/\sqrt{2}  & 1/\sqrt{2}  \\-1/\sqrt{2} & 1/\sqrt{2}\end{pmatrix}\begin{pmatrix}6/5 & 4/5 \\4/5 & 6/5\end{pmatrix}\begin{pmatrix}1/\sqrt{2} & -1/\sqrt{2}  \\1/\sqrt{2} & 1/\sqrt{2}\end{pmatrix}=\begin{pmatrix}2 & 0  \\0 & 2/5\end{pmatrix}$$

要使二维降到一维，去`P`中的第一行作为新基，就得到这些点降维后的表示：

$$Y=\begin{pmatrix}1/\sqrt{2} & 1/\sqrt{2}\end{pmatrix}\begin{pmatrix}-1 & -1 & 0 & 2 & 0 \\-2 & 0 & 0 & 1 & 1\end{pmatrix}=\begin{pmatrix}-3/\sqrt{2} & -1/\sqrt{2} & 0 & 3/\sqrt{2} & -1/\sqrt{2}\end{pmatrix}$$

![](/images/pca/1-8.png)

## [](#局限 "局限")局限

`PCA`的优势和弱点都在于它的`无参数`分析，它的步骤很固定，也不需要调参之类的，当然这也成了它的局限性。拓展可以考虑通过`Kernel`函数将非线性相关转为线性相关。有兴趣可以查阅相关论文。

参考资料：
1.[A Tutorial on Principal Component Analysis](https://www.cs.cmu.edu/~elaw/papers/pca.pdf) ,Jonathon Shlens
2.[PCA的数学原理](http://blog.codinglabs.org/articles/pca-tutorial.html)
3.[Principal component analysis with linear algebra](http://www.math.union.edu/~jaureguj/PCA.pdf) ,Jeff Jauregui